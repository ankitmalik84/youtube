# üöÄ Transformers Low-Level API Tutorial

## üìñ Overview

This notebook explores the lower-level API of Hugging Face Transformers, focusing on efficient model loading and quantization techniques. Learn how to run large language models like Llama, Phi3, and Gemma on consumer hardware using advanced memory optimization.

## ‚ú® Features

- **4-bit Quantization**: Reduce memory usage by up to 75% with BitsAndBytesConfig
- **Multi-Model Support**: Works with Llama 3.1, Phi3, and Gemma2 models
- **Memory Optimization**: Efficient GPU memory management and cleanup
- **Streaming Output**: Real-time text generation with TextStreamer
- **Architecture Exploration**: Deep dive into transformer model internals
- **Production Ready**: Clean, reusable code patterns for AI applications

## üõ†Ô∏è Requirements

### Hardware

- **GPU**: NVIDIA GPU with at least 6GB VRAM (T4 or better)
- **RAM**: 8GB+ system RAM
- **Storage**: 10GB+ free space for model downloads

### Software

- Python 3.8+
- CUDA-compatible GPU drivers
- Hugging Face account (for gated models)

## üì¶ Installation

The notebook handles all dependencies automatically. Key packages include:

```bash
pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124
pip install bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0
```

## üöÄ Quick Start

1. **Clone or download** this notebook
2. **Set up your environment** in Google Colab or local Jupyter
3. **Add your Hugging Face token** to access gated models
4. **Run the cells** sequentially to explore different models

## üí° Key Concepts Explained

### BitsAndBytesConfig

Powerful quantization configuration for memory optimization:

```python
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # Use 4-bit precision
    bnb_4bit_use_double_quant=True,       # Double quantization
    bnb_4bit_compute_dtype=torch.bfloat16, # Computation dtype
    bnb_4bit_quant_type="nf4"            # Quantization algorithm
)
```

**Benefits:**

- üî• **75% memory reduction** compared to full precision
- ‚ö° **Faster inference** on consumer GPUs
- üéØ **Minimal quality loss** with NF4 quantization

### AutoModelForCausalLM

Unified interface for loading language models:

```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",              # Automatic device placement
    quantization_config=quant_config # Apply quantization
)
```

### Generate Function

Complete text generation pipeline with streaming:

```python
def generate(model, messages):
    # Tokenization with proper padding
    tokenizer = AutoTokenizer.from_pretrained(model)
    tokenizer.pad_token = tokenizer.eos_token

    # Chat template formatting
    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to("cuda")

    # Streaming setup
    streamer = TextStreamer(tokenizer)

    # Model loading and generation
    model = AutoModelForCausalLM.from_pretrained(
        model,
        device_map="auto",
        quantization_config=quant_config
    )

    outputs = model.generate(
        inputs,
        max_new_tokens=80,
        streamer=streamer
    )

    # Memory cleanup
    del model, inputs, tokenizer, outputs, streamer
    gc.collect()
    torch.cuda.empty_cache()
```

## ü§ñ Supported Models

| Model                     | Size        | Provider  | Memory (4-bit) |
| ------------------------- | ----------- | --------- | -------------- |
| **Llama 3.1 8B Instruct** | 8B params   | Meta      | ~5.6GB         |
| **Phi-3 Mini 4K**         | 3.8B params | Microsoft | ~2.8GB         |
| **Gemma 2 2B IT**         | 2B params   | Google    | ~1.8GB         |

## üîß Usage Examples

### Basic Text Generation

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Explain quantum computing in simple terms"}
]

generate("meta-llama/Meta-Llama-3.1-8B-Instruct", messages)
```

### Model Comparison

```python
# Test the same prompt across different models
models = [
    "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "microsoft/Phi-3-mini-4k-instruct",
    "google/gemma-2-2b-it"
]

for model in models:
    print(f"\n=== {model} ===")
    generate(model, messages)
```

### Memory Monitoring

```python
# Check memory usage
memory = model.get_memory_footprint() / 1e6
print(f"Memory footprint: {memory:,.1f} MB")
```

## üèóÔ∏è Architecture Deep Dive

The notebook explores transformer internals:

- **Embedding Layer**: Text ‚Üí Vector conversion
- **32 Decoder Layers**: Sequential processing with attention and MLP
- **Self-Attention**: Understanding word relationships
- **Layer Normalization**: Training stability
- **LM Head**: Vector ‚Üí Text probability conversion

## üìä Performance Metrics

### Memory Usage Comparison

| Precision   | Memory | Quality | Speed       |
| ----------- | ------ | ------- | ----------- |
| FP32 (Full) | 32GB   | 100%    | Baseline    |
| FP16        | 16GB   | 99.9%   | 1.5x faster |
| 4-bit (NF4) | 6GB    | 99.5%   | 2x faster   |

### Model Loading Times

- **First time**: 2-5 minutes (download + load)
- **Subsequent runs**: 30-60 seconds (load only)
- **With caching**: 10-20 seconds

## üêõ Troubleshooting

### Common Issues

**Out of Memory Errors**

```python
# Solution: Use device_map or smaller model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # Automatic GPU/CPU split
    quantization_config=quant_config
)
```

**Slow Loading**

- Expected for large models (multi-GB downloads)
- Use local caching for repeated runs
- Consider smaller models for development

**Access Denied**

- Accept model terms on Hugging Face Hub
- Ensure valid HF_TOKEN is set
- Check model availability in your region

**Tokenizer Warnings**

```python
# Solution: Always set pad token
tokenizer.pad_token = tokenizer.eos_token
```

## üéØ Best Practices

1. **Start with 4-bit quantization** for optimal memory/quality balance
2. **Use device_map="auto"** for automatic optimization
3. **Enable streaming** for better user experience
4. **Clean memory** between model loads
5. **Set pad tokens** to avoid warnings
6. **Use chat templates** for proper formatting

## üîç Advanced Topics

### Custom Quantization

```python
# 8-bit quantization for higher quality
quant_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16
)
```

### Batch Processing

```python
# Process multiple conversations
conversations = [
    [{"role": "user", "content": "Question 1"}],
    [{"role": "user", "content": "Question 2"}]
]

for conv in conversations:
    generate(model_name, conv)
```

### Model Comparison Framework

```python
def compare_models(prompt, models):
    results = {}
    for model in models:
        start_time = time.time()
        generate(model, prompt)
        results[model] = time.time() - start_time
    return results
```

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- Additional model support
- Performance optimizations
- Better error handling
- Extended documentation
- More usage examples

## üîó Related Resources

- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)
- [BitsAndBytes GitHub](https://github.com/TimDettmers/bitsandbytes)
- [Model Cards on Hugging Face Hub](https://huggingface.co/models)
- [CUDA Installation Guide](https://developer.nvidia.com/cuda-downloads)

---

**‚≠ê If this notebook helped you understand transformer internals, please star the repository and share with the community!**

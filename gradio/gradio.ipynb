{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\yt\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI.\"\n",
    "system_message += \"Give Short, courteous answers. no more than 1 sentence.\"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    messages = [{\"role\":\"system\",\"content\":system_message}] + history + [{\"role\":\"user\",\"content\":message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant for an Airline called FlightAI.Give Short, courteous answers. no more than 1 sentence.Always be accurate. If you don't know the answer, say so.\"}, {'role': 'user', 'content': 'hi'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OPTIONS = {\n",
    "    \"OpenAI GPT-4o-mini\": \"openai:gpt-4o-mini\",\n",
    "    \"LLAMA3.2 (Ollama)\": \"ollama:llama3.2:1b\",\n",
    "    \"DeepSeek-R1 1.5B (Ollama)\": \"ollama:deepseek-r1:1.5b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(messages, model_identifier):\n",
    "    \"\"\"\n",
    "    Gets a streaming response from the selected model.\n",
    "    This function now yields individual tokens for a smooth UI update.\n",
    "    \"\"\"\n",
    "    provider, model_name = model_identifier.split(\":\", 1)\n",
    "\n",
    "    if provider == \"openai\":\n",
    "        client = OpenAI() # Corrected from OpenAI()\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            token = chunk.choices[0].delta.content or \"\"\n",
    "            yield token\n",
    "\n",
    "    elif provider == \"ollama\":\n",
    "        url = \"http://localhost:11434/api/chat\"\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": True\n",
    "        }\n",
    "        try:\n",
    "            with requests.post(url, json=payload, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                for line in r.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line.decode(\"utf-8\"))\n",
    "                        token = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                        yield token\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            yield f\"Error: Could not connect to Ollama. {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_wrapper(message,history,model_choice):\n",
    "    app_history = [{'role':'system','content':system_message}]\n",
    "    for user_msg,ai_msg in history:\n",
    "        app_history.append({'role':'user','content':user_msg})\n",
    "        if ai_msg:  \n",
    "            app_history.append({'role':'assistant','content':ai_msg})\n",
    "\n",
    "    app_history.append({'role':'user','content':message})\n",
    "\n",
    "    api_model_identifier = MODEL_OPTIONS[model_choice]\n",
    "\n",
    "    history.append((message,\"\"))\n",
    "\n",
    "    responss_stream = get_model_response(app_history,api_model_identifier)\n",
    "\n",
    "    full_response = \"\"\n",
    "    for token in responss_stream:\n",
    "        full_response += token\n",
    "        history[-1] = (message,full_response)\n",
    "        yield history\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_2288\\839480612.py:10: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_2288\\839480612.py:10: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
    "    gr.Markdown(\"FlightAI Chatbot\")\n",
    "\n",
    "    model_dropdown = gr.Dropdown(\n",
    "        choices=list(MODEL_OPTIONS.keys()),\n",
    "        value=\"OpenAI GPT-4o-mini\",\n",
    "        label=\"Select Model\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id='chatbot',\n",
    "        label='chat',\n",
    "        bubble_full_width=False,\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    textbox = gr.Textbox(\n",
    "        placeholder=\"Ask something about FlightAI\",\n",
    "        label=\"Enter your message\",\n",
    "        scale=7\n",
    "    )\n",
    "\n",
    "    textbox.submit(\n",
    "        fn=chat_wrapper,\n",
    "        inputs = [textbox,chatbot,model_dropdown],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "\n",
    "    textbox.submit(lambda:\"\",inputs=[],outputs=[textbox])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
